{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from transformers import GPT2Tokenizer\n",
    "import regex\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_files(directory):\n",
    "    all_texts = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".xml\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            all_texts.extend(load_single_file(file_path))\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_file(file_path):\n",
    "    texts = []\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Navigate to the <body> and <p> tags\n",
    "        body = root.find(\".//body\")\n",
    "        if body:\n",
    "            for paragraph in body.findall(\".//p\"):\n",
    "                if paragraph.text:\n",
    "                    texts.append(paragraph.text.strip())\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replacing multiple spaces with one\n",
    "    text = regex.sub(r\"\\s+\", \" \", text)\n",
    "    # Keep Devanagari and spaces\n",
    "    text = regex.sub(r\"[^\\p{Devanagari}\\s]\", \"\", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    return [clean_text(text) for text in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(cleaned_dataset):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    # Set the padding token to be the same as eos_token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize the cleaned dataset\n",
    "    tokenized_data = [tokenizer.encode(text, truncation=True, padding=\"max_length\", max_length=512) for text in cleaned_dataset]\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First cleaned sample: हुनक बेसी भाग महत्त्वपूर्ण कथा सब माटिसँ एहि तरहेँ जुड़ल अछि जे ओहि कथाक पात्र सब क्षेत्रीय नहि रहि जाइछ अवं पाठक अनुभव करैछ जे क्षेत्र और वातावर्णक किंचित परिवर्तनक पश्चात् ओ ग्रामीण भारतक कोनो भागक कथा भ सकैछ एक साधारण मनुष्य मृत्यु सँ संघर्ष करैत और अपन वर्तमान समयकेँ सार्थक करबाक प्रयासमे आदर्शरूप बनि जाइछ पौषी लक्ष्मी देवी लक्ष्मी जिनक आराधना पौष मासमे कैल जाइछक मुकुन्द पाल वैह आदिरूपक महत्ताकेँ प्राप्त करैछ ओ एक वृद्ध कृषक अछि आ अपन क्षेत्रमे सभसँ नीक अछि ई सन् १९४४ ई थिक बंगालक भयंकर चक्रवात और अकालक बादक साल एहि साल खेत सभ धानसँ भरल अछि जकरा शीघ्रहि काटब आवश्यक अछि दोसर विश्वयुद्ध चलि रहल अछि तैं खेतिहर मजदूर मुश्किलसँ भेटैछ और ओहि क्षेत्रक खेतिहर मजदूर सब विभिन्न युद्धनिर्माण कार्यमे नियुक्त भ गेल अछि पाकल और स्वर्णिम धान मुकुन्दक लेल लक्ष्मी अछि सम्पत्तिक प्रतीक मात्र पराक्रमी लक्ष्मीकेँ प्राप्त क सकैछ मुकुन्दकेँ अपन शक्ति क्षीणताक अनुभव होइछ अपन समयक प्रसिद्ध फसल कटनिहार एहि यथार्थकेँ स्वीकार नहि करैछ जे ओ वृद्धक संगहि अशक्त सेहो भ गेल अछि ओ ताकत जुटैबाक लेल मदिरा पान करैत अछि और पैशाचिक शक्तिसँ फसल काटि लैत अछि जखन धानसँ लदल गाड़ीकेँ ओकर बड़द नहि उठा सकैछ तखन मुकुन्द गाड़ीक तरमे कान्ह लगा कय गाड़ी ठेलबाक प्रयास करैछ ओहि भारसँ दबि कयओ मरि जाइत अछिधानक शीशकेँ पकड़ ने और स्वर्णिम खेतक अन्तिम दर्शन करैत ओ लक्ष्मी केँ नहि जीति सकल\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    directory = \"Maithili\"\n",
    "    \n",
    "    raw_texts = load_all_files(directory)\n",
    "\n",
    "    cleaned_texts = clean_dataset(raw_texts)\n",
    "\n",
    "    print(\"First cleaned sample:\", cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokenized sample: [11976, 117, 24231, 223, 11976, 101, 11976, 243, 28225, 105, 24231, 229, 11976, 116, 24231, 222, 28225, 255, 48077, 11976, 245, 28225, 106, 11976, 117, 11976, 97, 24231, 235, 11976, 97, 24231, 235, 11976, 113, 11976, 103, 24231, 224, 11976, 108, 24231, 235, 11976, 96, 28225, 243, 11976, 98, 48077, 28225, 116, 11976, 105, 28225, 106, 48077, 11976, 253, 11976, 123, 11976, 116, 11976, 223, 28225, 237, 11976, 117, 11976, 123, 28225, 97, 11976, 108, 11976, 117, 24231, 229, 11976, 223, 28225, 250, 24231, 223, 11976, 94, 11976, 120, 11976, 110, 28225, 227, 11976, 249, 11976, 123, 28225, 250, 24231, 229, 28225, 241, 11976, 117, 11976, 123, 28225, 243, 11976, 98, 48077, 11976, 243, 28225, 103, 48077, 11976, 97, 24231, 235, 11976, 108, 28225, 116, 11976, 105, 28225, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 222, 11976, 107, 28225, 101, 11976, 117, 11976, 123, 28225, 108, 11976, 117, 11976, 123, 28225, 250, 48077, 11976, 229, 11976, 249, 28225, 227, 11976, 113, 11976, 224, 28225, 103, 48077, 11976, 254, 11976, 243, 28225, 227, 11976, 101, 24231, 223, 11976, 255, 11976, 113, 28225, 243, 11976, 108, 24231, 230, 11976, 249, 28225, 250, 24231, 229, 28225, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 28225, 242, 11976, 108, 28225, 113, 48077, 11976, 97, 48077, 11976, 113, 11976, 108, 24231, 235, 11976, 96, 11976, 243, 28225, 243, 11976, 123, 11976, 224, 11976, 248, 11976, 123, 11976, 97, 28225, 103, 11976, 108, 11976, 123, 11976, 113, 11976, 108, 24231, 235, 11976, 97, 11976, 101, 11976, 243, 28225, 103, 11976, 114, 24231, 235, 11976, 248, 48077, 11976, 97, 24231, 235, 28225, 241, 28225, 245, 24231, 235, 11976, 108, 48077, 11976, 106, 24231, 222, 11976, 96, 28225, 255, 48077, 11976, 108, 11976, 97, 11976, 243, 28225, 243, 24231, 233, 11976, 101, 24231, 233, 28225, 255, 48077, 11976, 245, 11976, 243, 28225, 243, 11976, 98, 48077, 28225, 255, 28225, 116, 11976, 243, 24231, 230, 11976, 249, 28225, 237, 11976, 243, 28225, 116, 48077, 11976, 100, 48077, 11976, 108, 11976, 96, 28225, 106, 11976, 101, 24231, 223, 11976, 115, 24231, 235, 11976, 107, 28225, 106, 24231, 225, 11976, 97, 24231, 235, 11976, 107, 24231, 223, 28225, 116, 11976, 223, 28225, 116, 11976, 224, 11976, 246, 11976, 108, 24231, 235, 11976, 115, 28225, 243, 11976, 108, 24231, 230, 11976, 97, 28225, 242, 11976, 108, 28225, 227, 11976, 103, 11976, 101, 28225, 113, 11976, 108, 24231, 235, 11976, 97, 11976, 106, 48077, 11976, 101, 28225, 116, 11976, 106, 11976, 107, 11976, 243, 24231, 229, 11976, 223, 28225, 116, 48077, 11976, 108, 24231, 235, 11976, 98, 11976, 243, 28225, 243, 11976, 108, 11976, 105, 48077, 11976, 243, 28225, 103, 24231, 235, 11976, 108, 11976, 107, 48077, 11976, 116, 11976, 106, 24231, 229, 28225, 228, 11976, 99, 11976, 108, 24231, 235, 11976, 114, 11976, 108, 24231, 224, 11976, 103, 28225, 105, 11976, 101, 11976, 123, 28225, 250, 48077, 11976, 229, 11976, 249, 28225, 103, 24231, 234, 11976, 115, 24231, 222, 28225, 110, 11976, 243, 24231, 235, 11976, 115, 24231, 235, 11976, 106, 24231]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = tokenize_dataset(cleaned_texts)\n",
    "\n",
    "print(\"First tokenized sample:\", tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized sample: [11976, 117, 24231, 223, 11976, 101, 11976, 243, 28225, 105, 24231, 229, 11976, 116, 24231, 222, 28225, 255, 48077, 11976, 245, 28225, 106, 11976, 117, 11976, 97, 24231, 235, 11976, 97, 24231, 235, 11976, 113, 11976, 103, 24231, 224, 11976, 108, 24231, 235, 11976, 96, 28225, 243, 11976, 98, 48077, 28225, 116, 11976, 105, 28225, 106, 48077, 11976, 253, 11976, 123, 11976, 116, 11976, 223, 28225, 237, 11976, 117, 11976, 123, 28225, 97, 11976, 108, 11976, 117, 24231, 229, 11976, 223, 28225, 250, 24231, 223, 11976, 94, 11976, 120, 11976, 110, 28225, 227, 11976, 249, 11976, 123, 28225, 250, 24231, 229, 28225, 241, 11976, 117, 11976, 123, 28225, 243, 11976, 98, 48077, 11976, 243, 28225, 103, 48077, 11976, 97, 24231, 235, 11976, 108, 28225, 116, 11976, 105, 28225, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 222, 11976, 107, 28225, 101, 11976, 117, 11976, 123, 28225, 108, 11976, 117, 11976, 123, 28225, 250, 48077, 11976, 229, 11976, 249, 28225, 227, 11976, 113, 11976, 224, 28225, 103, 48077, 11976, 254, 11976, 243, 28225, 227, 11976, 101, 24231, 223, 11976, 255, 11976, 113, 28225, 243, 11976, 108, 24231, 230, 11976, 249, 28225, 250, 24231, 229, 28225, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 28225, 242, 11976, 108, 28225, 113, 48077, 11976, 97, 48077, 11976, 113, 11976, 108, 24231, 235, 11976, 96, 11976, 243, 28225, 243, 11976, 123, 11976, 224, 11976, 248, 11976, 123, 11976, 97, 28225, 103, 11976, 108, 11976, 123, 11976, 113, 11976, 108, 24231, 235, 11976, 97, 11976, 101, 11976, 243, 28225, 103, 11976, 114, 24231, 235, 11976, 248, 48077, 11976, 97, 24231, 235, 28225, 241, 28225, 245, 24231, 235, 11976, 108, 48077, 11976, 106, 24231, 222, 11976, 96, 28225, 255, 48077, 11976, 108, 11976, 97, 11976, 243, 28225, 243, 24231, 233, 11976, 101, 24231, 233, 28225, 255, 48077, 11976, 245, 11976, 243, 28225, 243, 11976, 98, 48077, 28225, 255, 28225, 116, 11976, 243, 24231, 230, 11976, 249, 28225, 237, 11976, 243, 28225, 116, 48077, 11976, 100, 48077, 11976, 108, 11976, 96, 28225, 106, 11976, 101, 24231, 223, 11976, 115, 24231, 235, 11976, 107, 28225, 106, 24231, 225, 11976, 97, 24231, 235, 11976, 107, 24231, 223, 28225, 116, 11976, 223, 28225, 116, 11976, 224, 11976, 246, 11976, 108, 24231, 235, 11976, 115, 28225, 243, 11976, 108, 24231, 230, 11976, 97, 28225, 242, 11976, 108, 28225, 227, 11976, 103, 11976, 101, 28225, 113, 11976, 108, 24231, 235, 11976, 97, 11976, 106, 48077, 11976, 101, 28225, 116, 11976, 106, 11976, 107, 11976, 243, 24231, 229, 11976, 223, 28225, 116, 48077, 11976, 108, 24231, 235, 11976, 98, 11976, 243, 28225, 243, 11976, 108, 11976, 105, 48077, 11976, 243, 28225, 103, 24231, 235, 11976, 108, 11976, 107, 48077, 11976, 116, 11976, 106, 24231, 229, 28225, 228, 11976, 99, 11976, 108, 24231, 235, 11976, 114, 11976, 108, 24231, 224, 11976, 103, 28225, 105, 11976, 101, 11976, 123, 28225, 250, 48077, 11976, 229, 11976, 249, 28225, 103, 24231, 234, 11976, 115, 24231, 222, 28225, 110, 11976, 243, 24231, 235, 11976, 115, 24231, 235, 11976, 106, 24231]\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokenized_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenized_texts, f)\n",
    "\n",
    "with open(\"tokenized_data.pkl\", \"rb\") as f:\n",
    "    loaded_tokenized_data = pickle.load(f)\n",
    "    print(\"Loaded tokenized sample:\", loaded_tokenized_data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
